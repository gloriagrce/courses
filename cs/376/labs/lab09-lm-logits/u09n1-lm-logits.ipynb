{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"authorship_tag":"ABX9TyOVes22Uvu4sPkNN1/P8/pg","collapsed_sections":[],"include_colab_link":true,"name":"013-lm-logits","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Logits in Causal Language Models\n\nTask: Ask a language model for how likely each token is to be the next one.","metadata":{"id":"3jc8Qlh1TEgC"}},{"cell_type":"markdown","source":"## Setup","metadata":{"id":"f8_8RWp3TX-8"}},{"cell_type":"markdown","source":"We start in the same way as the tokenization notebook:","metadata":{}},{"cell_type":"code","source":"# If the import fails, uncomment the following line:\n# !pip install transformers\nimport torch\nfrom torch import tensor\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport pandas as pd\n# Avoid a warning message\nimport os; os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"id":"osKgPaDwhaN4","execution":{"iopub.status.busy":"2025-05-04T01:51:59.127269Z","iopub.execute_input":"2025-05-04T01:51:59.127485Z","iopub.status.idle":"2025-05-04T01:52:05.851391Z","shell.execute_reply.started":"2025-05-04T01:51:59.127464Z","shell.execute_reply":"2025-05-04T01:52:05.850712Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"One step in this notebook will ask you to write a function. The most common error when function-ifying notebook code is accidentally using a global variable instead of a value computed in the function. This is a quick and dirty little utility to check for that mistake. (For a more polished version, check out [`localscope`](https://localscope.readthedocs.io/en/latest/README.html).)","metadata":{}},{"cell_type":"code","source":"def check_global_vars(func, allowed_globals):\n    import inspect\n    used_globals = set(inspect.getclosurevars(func).globals.keys())\n    disallowed_globals = used_globals - set(allowed_globals)\n    if len(disallowed_globals) > 0:\n        raise AssertionError(f\"The function {func.__name__} used unexpected global variables: {list(disallowed_globals)}\")","metadata":{"execution":{"iopub.status.busy":"2025-05-04T01:52:05.853153Z","iopub.execute_input":"2025-05-04T01:52:05.853505Z","iopub.status.idle":"2025-05-04T01:52:05.858155Z","shell.execute_reply.started":"2025-05-04T01:52:05.853484Z","shell.execute_reply":"2025-05-04T01:52:05.857219Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Download and load the model.","metadata":{"id":"UiNKbIh8hyDg"}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\nmodel_name = \"openai-community/gpt2\"\n# Here's a few larger models you could try:\n# model_name = \"EleutherAI/pythia-1.4b-deduped\"\n# model_name = \"google/gemma-2b\"\n# model_name = \"google/gemma-2b-it\"\n# Note: you'll need to accept the license agreement on https://huggingface.co/google/gemma-7b to use Gemma models\ntokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n\n# add the EOS token as PAD token to avoid warnings\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nif model.generation_config.pad_token_id is None:\n    model.generation_config.pad_token_id = model.generation_config.eos_token_id\n# Silence a warning.\ntokenizer.decode([tokenizer.eos_token_id]);","metadata":{"id":"IM5o_4w1hfyV","execution":{"iopub.status.busy":"2025-05-04T01:52:05.859124Z","iopub.execute_input":"2025-05-04T01:52:05.859345Z","iopub.status.idle":"2025-05-04T01:52:21.649095Z","shell.execute_reply.started":"2025-05-04T01:52:05.859326Z","shell.execute_reply":"2025-05-04T01:52:21.648154Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"697db22ccc0a4ff38bc8c25caa505645"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30f38fdd27d541329748bcd44b7e386f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39da7e4233414844ba52d34323b0e2ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f56b0e5b256e416980a652dfd50a1222"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c110235e3a84e2db428e844a4b43d39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fa360db9e334b7b9d4de57e35890342"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22f20ba7f8e3498eaf9be810907d8f2a"}},"metadata":{}},{"name":"stderr","text":"2025-05-04 01:52:13.461400: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-05-04 01:52:13.461559: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-05-04 01:52:13.615985: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print(f\"The tokenizer has {len(tokenizer.get_vocab())} strings in its vocabulary.\")\nprint(f\"The model has {model.num_parameters():,d} parameters.\")","metadata":{"id":"m-Z9_U0LUEVQ","outputId":"cb7d4eb7-bc54-4583-dd10-e0cb185494da","execution":{"iopub.status.busy":"2025-05-04T01:52:21.650260Z","iopub.execute_input":"2025-05-04T01:52:21.651185Z","iopub.status.idle":"2025-05-04T01:52:21.685027Z","shell.execute_reply.started":"2025-05-04T01:52:21.651159Z","shell.execute_reply":"2025-05-04T01:52:21.684101Z"},"trusted":true},"outputs":[{"name":"stdout","text":"The tokenizer has 50257 strings in its vocabulary.\nThe model has 124,439,808 parameters.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Task\n\nIn the tokenization notebook, we simply used the `generate` method to have the model generate some text. Now we'll do it ourselves.\n\nConsider the following phrase:","metadata":{"id":"OOUiz_PsUZgS"}},{"cell_type":"code","source":"phrase = \"This weekend I plan to\"\n# Another one to try later. This was a famous early example of the GPT-2 model:\n# phrase = \"In a shocking finding, scientists discovered a herd of unicorns living in\"","metadata":{"id":"JS7Z-DjoUiLK","execution":{"iopub.status.busy":"2025-05-04T01:52:21.687174Z","iopub.execute_input":"2025-05-04T01:52:21.687452Z","iopub.status.idle":"2025-05-04T01:52:21.756810Z","shell.execute_reply.started":"2025-05-04T01:52:21.687431Z","shell.execute_reply":"2025-05-04T01:52:21.755801Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"1: Call the `tokenizer` on the phrase to get a `batch`. After having a look at what goes in the `batch`, extract the `input_ids`.","metadata":{}},{"cell_type":"code","source":"batch = tokenizer(phrase, return_tensors='pt')\ninput_ids = batch['input_ids']\ninput_ids","metadata":{"id":"QpyeKakrjfpt","execution":{"iopub.status.busy":"2025-05-04T01:52:21.757864Z","iopub.execute_input":"2025-05-04T01:52:21.758130Z","iopub.status.idle":"2025-05-04T01:52:21.780679Z","shell.execute_reply.started":"2025-05-04T01:52:21.758108Z","shell.execute_reply":"2025-05-04T01:52:21.779669Z"},"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"tensor([[ 770, 5041,  314, 1410,  284]])"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"2: Call the `model` on the `input_ids`. Examine the shape of the logits; what does each number mean?\n\nNote: The `model` returns an object that has multiple values. The `logits` are in `model_output.logits`.","metadata":{}},{"cell_type":"code","source":"with torch.no_grad(): # This tells PyTorch we don't need it to compute gradients for us.\n    model_output = model(input_ids)\nprint(f\"logits shape: {list(model_output.logits.shape)}\")","metadata":{"id":"ZEy1QBTDotjU","outputId":"70a3312b-1bc7-47b0-c1d6-4739c93f56c0","execution":{"iopub.status.busy":"2025-05-04T01:52:21.781716Z","iopub.execute_input":"2025-05-04T01:52:21.781969Z","iopub.status.idle":"2025-05-04T01:52:21.971055Z","shell.execute_reply.started":"2025-05-04T01:52:21.781949Z","shell.execute_reply":"2025-05-04T01:52:21.970023Z"},"trusted":true},"outputs":[{"name":"stdout","text":"logits shape: [1, 5, 50257]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"Each number mean:\n1. The number 1 represent: the batch size.\n2. The number 5 represent: The amount of number in the phrase.\n3. The number 50257 represent: The total tokens that is in the vocabulary. ","metadata":{}},{"cell_type":"markdown","source":"3: Pull out the logits corresponding to the *last* token in the input phrase. Hint: Think about what each number in the shape means.","metadata":{}},{"cell_type":"code","source":"last_token_logits = model_output.logits[0,-1]\nassert last_token_logits.shape == (len(tokenizer.get_vocab()),)","metadata":{"execution":{"iopub.status.busy":"2025-05-04T01:52:21.972154Z","iopub.execute_input":"2025-05-04T01:52:21.972421Z","iopub.status.idle":"2025-05-04T01:52:22.001900Z","shell.execute_reply.started":"2025-05-04T01:52:21.972399Z","shell.execute_reply":"2025-05-04T01:52:22.000785Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"4: Identify the token id and corresponding string of the most likely next token.\n\nTo find the most likely token, we need to find the *index* of the *largest value* in the `last_token_logits`. The method that does this is called `argmax`. (It's a common enough operation that it's built into PyTorch.)\n\nNote: The `tokenizer` has a `decode` method that takes a token id, or a list of token ids, and returns the corresponding string.","metadata":{}},{"cell_type":"code","source":"# compute the probability distribution over the next token\nlast_token_probabilities = last_token_logits.softmax(dim=-1)\n# dim=-1 means to compute the softmax over the last dimension","metadata":{"execution":{"iopub.status.busy":"2025-05-04T01:52:22.002927Z","iopub.execute_input":"2025-05-04T01:52:22.003170Z","iopub.status.idle":"2025-05-04T01:52:22.018669Z","shell.execute_reply.started":"2025-05-04T01:52:22.003149Z","shell.execute_reply":"2025-05-04T01:52:22.017740Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"most_likely_token_id = last_token_probabilities.argmax()\ndecoded_token = tokenizer.decode(most_likely_token_id)\nprobability_of_most_likely_token = last_token_probabilities[most_likely_token_id]\n\nprint(\"For the phrase:\", phrase)\nprint(f\"Most likely next token: {most_likely_token_id}, which corresponds to {repr(decoded_token)}, with probability {probability_of_most_likely_token:.2%}\")","metadata":{"execution":{"iopub.status.busy":"2025-05-04T01:52:22.019837Z","iopub.execute_input":"2025-05-04T01:52:22.020203Z","iopub.status.idle":"2025-05-04T01:52:22.037149Z","shell.execute_reply.started":"2025-05-04T01:52:22.020175Z","shell.execute_reply":"2025-05-04T01:52:22.036272Z"},"trusted":true},"outputs":[{"name":"stdout","text":"For the phrase: This weekend I plan to\nMost likely next token: 467, which corresponds to ' go', with probability 5.79%\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"5: Use the `topk` method to find the top-10 most likely choices for the next token.\n\nSee the documentation for [`torch.topk`](https://pytorch.org/docs/stable/generated/torch.topk.html). Calling `topk` on a tensor returns a named tuple with two tensors: `values` and `indices`. The `values` are the top-k values, and the `indices` are the indices of those values in the original tensor. (In this case, the indices are the token ids.)\n\n*Note*: This uses Pandas to make a nicely displayed table, and a *list comprehension* to decode the tokens. You don't *need* to understand how this all works, but I highly encourage thinking about what's going on.","metadata":{}},{"cell_type":"code","source":"most_likely_tokens = last_token_logits.topk(10)\nprint(f\"most likely token index from topk is {most_likely_tokens.indices[0]}\") # this should be the same as argmax\ndecoded_tokens = [tokenizer.decode(hi) for hi in most_likely_tokens.indices]\nprobabilities_of_most_likely_tokens = last_token_probabilities[most_likely_tokens.indices]\n\n# Make a nice table to show the results\nmost_likely_tokens_df = pd.DataFrame({\n    'tokens': decoded_tokens,\n    'probabilities': probabilities_of_most_likely_tokens,\n})\n# Show the table, in a nice formatted way (see https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html#Builtin-Styles)\n# Caution: this \"gradient\" has *nothing* to do with gradient descent! (It's a color gradient.)\nmost_likely_tokens_df.style.hide(axis='index').background_gradient()","metadata":{"execution":{"iopub.status.busy":"2025-05-04T01:52:22.038259Z","iopub.execute_input":"2025-05-04T01:52:22.038638Z","iopub.status.idle":"2025-05-04T01:52:22.125797Z","shell.execute_reply.started":"2025-05-04T01:52:22.038616Z","shell.execute_reply":"2025-05-04T01:52:22.124785Z"},"trusted":true},"outputs":[{"name":"stdout","text":"most likely token index from topk is 467\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7cc695cb1c00>","text/html":"<style type=\"text/css\">\n#T_1e236_row0_col1 {\n  background-color: #023858;\n  color: #f1f1f1;\n}\n#T_1e236_row1_col1 {\n  background-color: #04598c;\n  color: #f1f1f1;\n}\n#T_1e236_row2_col1 {\n  background-color: #6ba5cd;\n  color: #f1f1f1;\n}\n#T_1e236_row3_col1 {\n  background-color: #83afd3;\n  color: #f1f1f1;\n}\n#T_1e236_row4_col1 {\n  background-color: #d2d3e7;\n  color: #000000;\n}\n#T_1e236_row5_col1 {\n  background-color: #e0deed;\n  color: #000000;\n}\n#T_1e236_row6_col1 {\n  background-color: #e7e3f0;\n  color: #000000;\n}\n#T_1e236_row7_col1 {\n  background-color: #f2ecf5;\n  color: #000000;\n}\n#T_1e236_row8_col1 {\n  background-color: #faf2f8;\n  color: #000000;\n}\n#T_1e236_row9_col1 {\n  background-color: #fff7fb;\n  color: #000000;\n}\n</style>\n<table id=\"T_1e236\">\n  <thead>\n    <tr>\n      <th id=\"T_1e236_level0_col0\" class=\"col_heading level0 col0\" >tokens</th>\n      <th id=\"T_1e236_level0_col1\" class=\"col_heading level0 col1\" >probabilities</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_1e236_row0_col0\" class=\"data row0 col0\" > go</td>\n      <td id=\"T_1e236_row0_col1\" class=\"data row0 col1\" >0.057942</td>\n    </tr>\n    <tr>\n      <td id=\"T_1e236_row1_col0\" class=\"data row1 col0\" > take</td>\n      <td id=\"T_1e236_row1_col1\" class=\"data row1 col1\" >0.053054</td>\n    </tr>\n    <tr>\n      <td id=\"T_1e236_row2_col0\" class=\"data row2 col0\" > attend</td>\n      <td id=\"T_1e236_row2_col1\" class=\"data row2 col1\" >0.038623</td>\n    </tr>\n    <tr>\n      <td id=\"T_1e236_row3_col0\" class=\"data row3 col0\" > visit</td>\n      <td id=\"T_1e236_row3_col1\" class=\"data row3 col1\" >0.036411</td>\n    </tr>\n    <tr>\n      <td id=\"T_1e236_row4_col0\" class=\"data row4 col0\" > be</td>\n      <td id=\"T_1e236_row4_col1\" class=\"data row4 col1\" >0.027353</td>\n    </tr>\n    <tr>\n      <td id=\"T_1e236_row5_col0\" class=\"data row5 col0\" > do</td>\n      <td id=\"T_1e236_row5_col1\" class=\"data row5 col1\" >0.024954</td>\n    </tr>\n    <tr>\n      <td id=\"T_1e236_row6_col0\" class=\"data row6 col0\" > make</td>\n      <td id=\"T_1e236_row6_col1\" class=\"data row6 col1\" >0.023816</td>\n    </tr>\n    <tr>\n      <td id=\"T_1e236_row7_col0\" class=\"data row7 col0\" > spend</td>\n      <td id=\"T_1e236_row7_col1\" class=\"data row7 col1\" >0.021304</td>\n    </tr>\n    <tr>\n      <td id=\"T_1e236_row8_col0\" class=\"data row8 col0\" > play</td>\n      <td id=\"T_1e236_row8_col1\" class=\"data row8 col1\" >0.019174</td>\n    </tr>\n    <tr>\n      <td id=\"T_1e236_row9_col0\" class=\"data row9 col0\" > travel</td>\n      <td id=\"T_1e236_row9_col1\" class=\"data row9 col1\" >0.017760</td>\n    </tr>\n  </tbody>\n</table>\n"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"6. Write a function that is given a phrase and a *k* and returns the `most_likely_tokens_df` DataFrame with the top *k* most likely next tokens. (Don't include the `style` line.)\n\nBuild this function using only code that you've already filled in above. Clean up the code so that it doesn't do or display anything extraneous. Add comments about what each step does.\n","metadata":{}},{"cell_type":"code","source":"def predict_next_tokens(phrase, k):\n    batch = tokenizer(phrase, return_tensors='pt')\n    input_ids = batch['input_ids']\n\n    with torch.no_grad():\n        model_output = model(input_ids)\n        \n    last_token_logits = model_output.logits[0, -3]\n    last_token_probabilities = last_token_logits.softmax(dim=-1)\n    \n    \n    most_likely_tokens = last_token_logits.topk(k)\n    most_likely_token_indices = most_likely_tokens.indices\n    decoded_tokens = [tokenizer.decode(hi) for hi in most_likely_token_indices]\n    probabilities_of_most_likely_tokens = last_token_probabilities[most_likely_token_indices]\n    \n    most_likely_tokens_df = pd.DataFrame({\n        'tokens': decoded_tokens,\n        'probabilities': probabilities_of_most_likely_tokens,\n    })\n\n    return most_likely_tokens_df\n\n\ndef show_tokens_df(tokens_df):\n    return tokens_df.style.hide(axis='index').background_gradient()\n\ncheck_global_vars(predict_next_tokens, allowed_globals=[\"torch\", \"tokenizer\", \"pd\", \"model\"])","metadata":{"id":"kJWXqQLLkCyP","outputId":"b969f2ed-2289-48c2-e717-b4802a493c5d","execution":{"iopub.status.busy":"2025-05-04T01:52:22.126884Z","iopub.execute_input":"2025-05-04T01:52:22.127279Z","iopub.status.idle":"2025-05-04T01:52:22.133606Z","shell.execute_reply.started":"2025-05-04T01:52:22.127252Z","shell.execute_reply":"2025-05-04T01:52:22.132550Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"show_tokens_df(predict_next_tokens(\"This weekend I plan to\", 5))","metadata":{"execution":{"iopub.status.busy":"2025-05-04T01:52:22.134771Z","iopub.execute_input":"2025-05-04T01:52:22.135013Z","iopub.status.idle":"2025-05-04T01:52:22.227259Z","shell.execute_reply.started":"2025-05-04T01:52:22.134994Z","shell.execute_reply":"2025-05-04T01:52:22.226240Z"},"trusted":true},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7cc695cb1870>","text/html":"<style type=\"text/css\">\n#T_8af48_row0_col1 {\n  background-color: #023858;\n  color: #f1f1f1;\n}\n#T_8af48_row1_col1 {\n  background-color: #b1c2de;\n  color: #000000;\n}\n#T_8af48_row2_col1 {\n  background-color: #e6e2ef;\n  color: #000000;\n}\n#T_8af48_row3_col1 {\n  background-color: #f9f2f8;\n  color: #000000;\n}\n#T_8af48_row4_col1 {\n  background-color: #fff7fb;\n  color: #000000;\n}\n</style>\n<table id=\"T_8af48\">\n  <thead>\n    <tr>\n      <th id=\"T_8af48_level0_col0\" class=\"col_heading level0 col0\" >tokens</th>\n      <th id=\"T_8af48_level0_col1\" class=\"col_heading level0 col1\" >probabilities</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_8af48_row0_col0\" class=\"data row0 col0\" > was</td>\n      <td id=\"T_8af48_row0_col1\" class=\"data row0 col1\" >0.107944</td>\n    </tr>\n    <tr>\n      <td id=\"T_8af48_row1_col0\" class=\"data row1 col0\" > will</td>\n      <td id=\"T_8af48_row1_col1\" class=\"data row1 col1\" >0.064832</td>\n    </tr>\n    <tr>\n      <td id=\"T_8af48_row2_col0\" class=\"data row2 col0\" >'ll</td>\n      <td id=\"T_8af48_row2_col1\" class=\"data row2 col1\" >0.052473</td>\n    </tr>\n    <tr>\n      <td id=\"T_8af48_row3_col0\" class=\"data row3 col0\" > had</td>\n      <td id=\"T_8af48_row3_col1\" class=\"data row3 col1\" >0.045030</td>\n    </tr>\n    <tr>\n      <td id=\"T_8af48_row4_col0\" class=\"data row4 col0\" > went</td>\n      <td id=\"T_8af48_row4_col1\" class=\"data row4 col1\" >0.042391</td>\n    </tr>\n  </tbody>\n</table>\n"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"show_tokens_df(predict_next_tokens(\"To be or not to\", 5))","metadata":{"execution":{"iopub.status.busy":"2025-05-04T01:52:22.229879Z","iopub.execute_input":"2025-05-04T01:52:22.230135Z","iopub.status.idle":"2025-05-04T01:52:22.309434Z","shell.execute_reply.started":"2025-05-04T01:52:22.230114Z","shell.execute_reply":"2025-05-04T01:52:22.308382Z"},"trusted":true},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7cc71105f940>","text/html":"<style type=\"text/css\">\n#T_70387_row0_col1 {\n  background-color: #023858;\n  color: #f1f1f1;\n}\n#T_70387_row1_col1 {\n  background-color: #f4eef6;\n  color: #000000;\n}\n#T_70387_row2_col1 {\n  background-color: #fbf3f9;\n  color: #000000;\n}\n#T_70387_row3_col1, #T_70387_row4_col1 {\n  background-color: #fff7fb;\n  color: #000000;\n}\n</style>\n<table id=\"T_70387\">\n  <thead>\n    <tr>\n      <th id=\"T_70387_level0_col0\" class=\"col_heading level0 col0\" >tokens</th>\n      <th id=\"T_70387_level0_col1\" class=\"col_heading level0 col1\" >probabilities</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_70387_row0_col0\" class=\"data row0 col0\" > not</td>\n      <td id=\"T_70387_row0_col1\" class=\"data row0 col1\" >0.749065</td>\n    </tr>\n    <tr>\n      <td id=\"T_70387_row1_col0\" class=\"data row1 col0\" > to</td>\n      <td id=\"T_70387_row1_col1\" class=\"data row1 col1\" >0.062378</td>\n    </tr>\n    <tr>\n      <td id=\"T_70387_row2_col0\" class=\"data row2 col0\" > be</td>\n      <td id=\"T_70387_row2_col1\" class=\"data row2 col1\" >0.030197</td>\n    </tr>\n    <tr>\n      <td id=\"T_70387_row3_col0\" class=\"data row3 col0\" > have</td>\n      <td id=\"T_70387_row3_col1\" class=\"data row3 col1\" >0.009974</td>\n    </tr>\n    <tr>\n      <td id=\"T_70387_row4_col0\" class=\"data row4 col0\" > never</td>\n      <td id=\"T_70387_row4_col1\" class=\"data row4 col1\" >0.007600</td>\n    </tr>\n  </tbody>\n</table>\n"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"show_tokens_df(predict_next_tokens(\"For God so loved the\", 5))","metadata":{"execution":{"iopub.status.busy":"2025-05-04T01:52:22.310635Z","iopub.execute_input":"2025-05-04T01:52:22.310945Z","iopub.status.idle":"2025-05-04T01:52:22.390554Z","shell.execute_reply.started":"2025-05-04T01:52:22.310922Z","shell.execute_reply":"2025-05-04T01:52:22.389589Z"},"trusted":true},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7cc71105fc70>","text/html":"<style type=\"text/css\">\n#T_93b3c_row0_col1 {\n  background-color: #023858;\n  color: #f1f1f1;\n}\n#T_93b3c_row1_col1 {\n  background-color: #eee9f3;\n  color: #000000;\n}\n#T_93b3c_row2_col1 {\n  background-color: #fef6fa;\n  color: #000000;\n}\n#T_93b3c_row3_col1 {\n  background-color: #fef6fb;\n  color: #000000;\n}\n#T_93b3c_row4_col1 {\n  background-color: #fff7fb;\n  color: #000000;\n}\n</style>\n<table id=\"T_93b3c\">\n  <thead>\n    <tr>\n      <th id=\"T_93b3c_level0_col0\" class=\"col_heading level0 col0\" >tokens</th>\n      <th id=\"T_93b3c_level0_col1\" class=\"col_heading level0 col1\" >probabilities</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_93b3c_row0_col0\" class=\"data row0 col0\" > loved</td>\n      <td id=\"T_93b3c_row0_col1\" class=\"data row0 col1\" >0.743034</td>\n    </tr>\n    <tr>\n      <td id=\"T_93b3c_row1_col0\" class=\"data row1 col0\" > help</td>\n      <td id=\"T_93b3c_row1_col1\" class=\"data row1 col1\" >0.091278</td>\n    </tr>\n    <tr>\n      <td id=\"T_93b3c_row2_col0\" class=\"data row2 col0\" > love</td>\n      <td id=\"T_93b3c_row2_col1\" class=\"data row2 col1\" >0.016821</td>\n    </tr>\n    <tr>\n      <td id=\"T_93b3c_row3_col0\" class=\"data row3 col0\" > that</td>\n      <td id=\"T_93b3c_row3_col1\" class=\"data row3 col1\" >0.016379</td>\n    </tr>\n    <tr>\n      <td id=\"T_93b3c_row4_col0\" class=\"data row4 col0\" > pleased</td>\n      <td id=\"T_93b3c_row4_col1\" class=\"data row4 col1\" >0.010759</td>\n    </tr>\n  </tbody>\n</table>\n"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"## Analysis\n","metadata":{"id":"PsI_Tz0ipglx"}},{"cell_type":"markdown","source":"Q1: Explain the shape of `model_output.logits`.\n\nEach number mean:\n1. The number 1 represent: the batch size.\n2. The number 5 represent: The amount of number in the phrase.\n3. The number 50257 represent: The total tokens that is in the vocabulary. ","metadata":{}},{"cell_type":"markdown","source":"Q2: Change the -1 in the definition of `last_token_logits` to -3. What does the variable represent now? What does its argmax represent?\n\nThe variable represent the entire sequence or each token in the sequence.\n\nThe argmax represents the index of the token with the highest probability for the third to last position in the input sequence. This means that the argmax will be the most likely token to replace the third to last token in the input sequence.\n","metadata":{}},{"cell_type":"markdown","source":"Q3: Let's think. The method in this notebook only get the scores for *one* next-token at a time. What if we wanted to do a whole sentence? Weâ€™d have to generate a token for each word in that sentence. What are a few different ways we could we adapt the approach used in this notebook to generate a complete sentence?\n\nTo think about different ways to do this, think about what decision(s) you have to make when generating each token.\n\nNote: you don't have to write any code to answer this question.\n\nI think one way to this approach can be selecting the most likely next token based on to the present sequence of tokens and keep going until the sequence reaches some kind of ending (reach the maximum or an ending such as . / ? / !\n\nAnother approach that is possible to do is to save of the list probable sequences at each step. By doing this, it can select the top k sequences from the expanded set of possible sequences. \n","metadata":{}}]}