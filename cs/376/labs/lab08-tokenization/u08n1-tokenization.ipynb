{"metadata":{"colab":{"authorship_tag":"ABX9TyNS7mRS03a7VSFcbdUnYf/k","collapsed_sections":[],"include_colab_link":true,"name":"012-tokenization.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tokenization\n\nTask: Convert text to numbers; interpret subword tokenization.\n\nThere are various different ways of converting text to numbers. This assignment works with one popular approach: assign numbers to parts of words.","metadata":{"id":"3jc8Qlh1TEgC"}},{"cell_type":"markdown","source":"## Setup","metadata":{"id":"f8_8RWp3TX-8"}},{"cell_type":"markdown","source":"We'll be using the HuggingFace Transformers library, which provides a (mostly) consistent interface to many different language models. We'll focus on the OpenAI GPT-2 model, famous for OpenAI's assertion that it was \"too dangerous\" to release in full.\n\n- [Documentation](https://huggingface.co/transformers/model_doc/gpt2.html) for the model and tokenizer.\n- [Model Card](https://github.com/openai/gpt-2/blob/master/model_card.md) for GPT-2.","metadata":{"id":"aUvTIxyWTdBF"}},{"cell_type":"markdown","source":"The `transformers` library is pre-installed on many systems, but in case you need to install it, you can run the following cell.","metadata":{}},{"cell_type":"code","source":"# Uncomment the following line to install the transformers library\n#!pip install -q transformers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vWy--2nwhWPy","outputId":"44b8e674-7e8b-4cf6-a1e9-1f8d62740382","execution":{"iopub.status.busy":"2024-03-17T03:56:07.194903Z","iopub.execute_input":"2024-03-17T03:56:07.195822Z","iopub.status.idle":"2024-03-17T03:56:07.200748Z","shell.execute_reply.started":"2024-03-17T03:56:07.195779Z","shell.execute_reply":"2024-03-17T03:56:07.199748Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import tensor","metadata":{"id":"osKgPaDwhaN4","execution":{"iopub.status.busy":"2024-03-17T03:56:07.202483Z","iopub.execute_input":"2024-03-17T03:56:07.203116Z","iopub.status.idle":"2024-03-17T03:56:10.100043Z","shell.execute_reply.started":"2024-03-17T03:56:07.203077Z","shell.execute_reply":"2024-03-17T03:56:10.099050Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Download and load the model\n\nThis cell downloads the model and tokenizer, and loads them into memory.","metadata":{"id":"UiNKbIh8hyDg"}},{"cell_type":"code","source":"# https://huggingface.co/docs/transformers/en/generation_strategies\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, set_seed\nmodel_name = \"openai-community/gpt2\"\n# Here's a few larger models you could try:\n# model_name = \"EleutherAI/pythia-1.4b-deduped\"\n# model_name = \"google/gemma-2b\"\n# model_name = \"google/gemma-2b-it\"\n# Note: you'll need to accept the license agreement on https://huggingface.co/google/gemma-7b to use Gemma models\ntokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n\n# add the EOS token as PAD token to avoid warnings\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nif model.generation_config.pad_token_id is None:\n    model.generation_config.pad_token_id = model.generation_config.eos_token_id\nstreamer = TextStreamer(tokenizer)\n# Silence a warning.\ntokenizer.decode([tokenizer.eos_token_id]);","metadata":{"id":"IM5o_4w1hfyV","execution":{"iopub.status.busy":"2024-03-17T03:56:10.101198Z","iopub.execute_input":"2024-03-17T03:56:10.101564Z","iopub.status.idle":"2024-03-17T03:56:28.986871Z","shell.execute_reply.started":"2024-03-17T03:56:10.101539Z","shell.execute_reply":"2024-03-17T03:56:28.985899Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caa947a957b94c36a55fc48656bd9e85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4f2336ba9d544c4ad36d51d035440ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ccd1aac72ea41ac8f9a1dae4fa3506d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55ad45ad958e4665b552a65b8ca42200"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7457d54543844255bf20f5a81e561dbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20a2097f84fb41bba9e813c763ceba66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65328755183b468ba6607a43e975dc65"}},"metadata":{}},{"name":"stderr","text":"2024-03-17 03:56:19.975319: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-17 03:56:19.975424: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-17 03:56:20.104708: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"token_to_id_dict = tokenizer.get_vocab()\nprint(f\"The tokenizer has {len(token_to_id_dict)} strings in its vocabulary.\")\nprint(f\"The model has {model.num_parameters():,d} parameters.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m-Z9_U0LUEVQ","outputId":"1d639faf-5b56-4bb2-81e5-054ee086ef0a","execution":{"iopub.status.busy":"2024-03-17T03:56:28.989175Z","iopub.execute_input":"2024-03-17T03:56:28.989807Z","iopub.status.idle":"2024-03-17T03:56:29.020458Z","shell.execute_reply.started":"2024-03-17T03:56:28.989779Z","shell.execute_reply":"2024-03-17T03:56:29.019555Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"The tokenizer has 50257 strings in its vocabulary.\nThe model has 124,439,808 parameters.\n","output_type":"stream"}]},{"cell_type":"code","source":"# warning: this assumes that there are no gaps in the token ids, which happens to be true for this tokenizer.\nid_to_token = [token for token, id in sorted(token_to_id_dict.items(), key=lambda x: x[1])]\nprint(f\"The first 10 tokens are: {id_to_token[:10]}\")\nprint(f\"The last 10 tokens are: {id_to_token[-10:]}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-17T03:56:29.021807Z","iopub.execute_input":"2024-03-17T03:56:29.022114Z","iopub.status.idle":"2024-03-17T03:56:29.084677Z","shell.execute_reply.started":"2024-03-17T03:56:29.022090Z","shell.execute_reply":"2024-03-17T03:56:29.083673Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"The first 10 tokens are: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\nThe last 10 tokens are: ['Ġ(/', 'âĢ¦.\"', 'Compar', 'Ġamplification', 'ominated', 'Ġregress', 'ĠCollider', 'Ġinformants', 'Ġgazed', '<|endoftext|>']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Demo","metadata":{}},{"cell_type":"code","source":"set_seed(0)\nmodel.generate(\n    **tokenizer(\"A list of colors: red, blue,\", return_tensors=\"pt\"),\n    max_new_tokens=10, do_sample=True, temperature=1.5, penalty_alpha=.5, top_k=5, streamer=streamer);","metadata":{"execution":{"iopub.status.busy":"2024-03-17T03:56:29.086048Z","iopub.execute_input":"2024-03-17T03:56:29.086700Z","iopub.status.idle":"2024-03-17T03:56:29.617586Z","shell.execute_reply.started":"2024-03-17T03:56:29.086662Z","shell.execute_reply":"2024-03-17T03:56:29.616535Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":" A list of colors: red, blue, purple, orange, and pink, as well as\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Task\n\nConsider the following phrase:","metadata":{"id":"OOUiz_PsUZgS"}},{"cell_type":"code","source":"phrase = \"I visited Muskegon\"\n# Another one to try later. This was a famous early example of the GPT-2 model:\n# phrase = \"In a shocking finding, scientists discovered a herd of unicorns living in\"","metadata":{"id":"JS7Z-DjoUiLK","execution":{"iopub.status.busy":"2024-03-17T03:56:29.618845Z","iopub.execute_input":"2024-03-17T03:56:29.619148Z","iopub.status.idle":"2024-03-17T03:56:29.623594Z","shell.execute_reply.started":"2024-03-17T03:56:29.619124Z","shell.execute_reply":"2024-03-17T03:56:29.622556Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Getting familiar with tokens\n\n1: Use `tokenizer.tokenize` to convert the phrase into a list of tokens. (What do you think the `Ġ` means?)","metadata":{}},{"cell_type":"code","source":"tokens = tokenizer.tokenize(phrase)\ntokens","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hyq-5XWSUx_8","outputId":"22efb7a8-37c5-46f0-e230-c3b8e5ad6bdc","execution":{"iopub.status.busy":"2024-03-17T03:56:29.625092Z","iopub.execute_input":"2024-03-17T03:56:29.625413Z","iopub.status.idle":"2024-03-17T03:56:29.640974Z","shell.execute_reply.started":"2024-03-17T03:56:29.625390Z","shell.execute_reply":"2024-03-17T03:56:29.640055Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['ĠI', 'Ġvisited', 'ĠMus', 'ke', 'gon']"},"metadata":{}}]},{"cell_type":"markdown","source":"2: Use `tokenizer.convert_tokens_to_string` to convert the tokens back into a string.\n","metadata":{}},{"cell_type":"code","source":"tokenizer.convert_tokens_to_string(tokens)","metadata":{"execution":{"iopub.status.busy":"2024-03-17T03:56:29.642252Z","iopub.execute_input":"2024-03-17T03:56:29.642508Z","iopub.status.idle":"2024-03-17T03:56:29.656378Z","shell.execute_reply.started":"2024-03-17T03:56:29.642487Z","shell.execute_reply":"2024-03-17T03:56:29.655553Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"' I visited Muskegon'"},"metadata":{}}]},{"cell_type":"code","source":"# for comparison:\n''.join(tokens)","metadata":{"execution":{"iopub.status.busy":"2024-03-17T03:56:29.659266Z","iopub.execute_input":"2024-03-17T03:56:29.659560Z","iopub.status.idle":"2024-03-17T03:56:29.673379Z","shell.execute_reply.started":"2024-03-17T03:56:29.659537Z","shell.execute_reply":"2024-03-17T03:56:29.672530Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'ĠIĠvisitedĠMuskegon'"},"metadata":{}}]},{"cell_type":"markdown","source":"What is the difference between the output from `convert_tokens_to_string` and the result of ''.join(tokens)?\n\nThe difference between the two output it the space is presented by Ġ. ","metadata":{}},{"cell_type":"markdown","source":"3: Use `tokenizer.encode` to convert the original phrase into token ids. (*Note: this is equivalent to `tokenize` followed by `convert_tokens_to_ids`*. Remember, **tokenizers have two jobs**; these correspond to the two methods.) Call the result `input_ids`.\n","metadata":{}},{"cell_type":"code","source":"input_ids =  tokenizer.encode(phrase)\ninput_ids","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GkaoLSFMiHzb","outputId":"18c6391e-a9aa-4d4c-dace-d49f8bbcba7a","execution":{"iopub.status.busy":"2024-03-17T03:56:29.674665Z","iopub.execute_input":"2024-03-17T03:56:29.675227Z","iopub.status.idle":"2024-03-17T03:56:29.693237Z","shell.execute_reply.started":"2024-03-17T03:56:29.675196Z","shell.execute_reply":"2024-03-17T03:56:29.692413Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[314, 8672, 2629, 365, 14520]"},"metadata":{}}]},{"cell_type":"markdown","source":"4: Turn `input_ids` back into a readable string. Try this two ways: (1) using `tokenizer.decode` and (2) in two steps: using `convert_ids_to_tokens`, then a second step that you've already done previously. **The result of (1) should be the same as the result of (2).**","metadata":{}},{"cell_type":"code","source":"# using convert_ids_to_tokens\ntoken2 = tokenizer.convert_ids_to_tokens(input_ids)\ntokenizer.convert_tokens_to_string(token2)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-17T03:56:29.694358Z","iopub.execute_input":"2024-03-17T03:56:29.694694Z","iopub.status.idle":"2024-03-17T03:56:29.711059Z","shell.execute_reply.started":"2024-03-17T03:56:29.694671Z","shell.execute_reply":"2024-03-17T03:56:29.709853Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"' I visited Muskegon'"},"metadata":{}}]},{"cell_type":"code","source":"# using tokenizer.decode\ntokenizer.decode(input_ids)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"ncSRaBaZix8R","outputId":"204670f9-d7c4-4856-c804-a038b77ccd1c","execution":{"iopub.status.busy":"2024-03-17T03:56:29.712521Z","iopub.execute_input":"2024-03-17T03:56:29.712800Z","iopub.status.idle":"2024-03-17T03:56:29.726721Z","shell.execute_reply.started":"2024-03-17T03:56:29.712779Z","shell.execute_reply":"2024-03-17T03:56:29.726012Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"' I visited Muskegon'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Applying what you learned\n\n5: Use `model.generate(input_ids_batch)` to generate a completion of this phrase. (Note that we needed to add `[]`s to give a \"batch\" dimension to the input, and convert the result to a PyTorch `tensor` for the model code to use it.) Call the result `output_ids`. This one is done for you.\n","metadata":{}},{"cell_type":"code","source":"input_ids_batch = tensor([input_ids])\noutput_ids = model.generate(input_ids_batch, max_new_tokens=20)\noutput_ids","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5PZm3eIjjKCJ","outputId":"1c4b1a63-de00-44f9-eee7-85714012dbc6","execution":{"iopub.status.busy":"2024-03-17T03:56:29.727760Z","iopub.execute_input":"2024-03-17T03:56:29.728039Z","iopub.status.idle":"2024-03-17T03:56:30.420206Z","shell.execute_reply.started":"2024-03-17T03:56:29.727988Z","shell.execute_reply":"2024-03-17T03:56:30.419271Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"tensor([[  314,  8672,  2629,   365, 14520,    11,   290,   314,   373,  1297,\n           326,   262,  1748,   373,   287,   262,  1429,   286,   852,  3170,\n            13,   314,   373,  1297,   326]])"},"metadata":{}}]},{"cell_type":"markdown","source":"6: Convert your `output_ids` into a readable form. (Note: it has an extra \"batch\" dimension, so you'll need to use `output_ids[0]`.)","metadata":{}},{"cell_type":"code","source":"readable_output = tokenizer.decode(output_ids[0])\nprint(readable_output)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"2kKJ8rvijVez","outputId":"386df167-0e88-45f1-b1a0-01beab1f0bc8","execution":{"iopub.status.busy":"2024-03-17T03:56:30.421380Z","iopub.execute_input":"2024-03-17T03:56:30.421728Z","iopub.status.idle":"2024-03-17T03:56:30.427943Z","shell.execute_reply.started":"2024-03-17T03:56:30.421704Z","shell.execute_reply":"2024-03-17T03:56:30.426895Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":" I visited Muskegon, and I was told that the city was in the process of being built. I was told that\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Note: `generate` uses a greedy decoding by default, but it's highly customizable. We'll play more with it in later exercises. For now, if you want more interesting results, try adding the following arguments to `generate`:\n\n- Turn on `do_sample=True`. Run it a few times to see what it gives. Try `temperature = 0.7` or `temperature = 1.5`.\n- With sampling enabled, set `top_k=5`. Or 50.","metadata":{}},{"cell_type":"markdown","source":"7. What is the largest possible token id for the tokenizer we're using in this notebook? **What token does it correspond to?** (Hint: at the top of the notebook, we printed out the size of the vocabulary.)","metadata":{}},{"cell_type":"code","source":"largest_token_id = len(tokenizer.get_vocab()) - 1\nlargest_token = tokenizer.decode(largest_token_id)\nprint(f\"The largest possible token ID is {largest_token_id}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-17T03:56:30.428982Z","iopub.execute_input":"2024-03-17T03:56:30.429315Z","iopub.status.idle":"2024-03-17T03:56:30.469904Z","shell.execute_reply.started":"2024-03-17T03:56:30.429292Z","shell.execute_reply":"2024-03-17T03:56:30.469015Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"The largest possible token ID is 50256\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Analysis","metadata":{}},{"cell_type":"markdown","source":"Q1: **Write a brief explanation of what a tokenizer does.** Note that we worked with two parts of a tokenizer in this exercise (one that deals only with strings, and another that deals with numbers); make sure your explanation addresses both parts.","metadata":{}},{"cell_type":"markdown","source":"A tokenizer is a processing systems that convert human language or text into tokens. The two parts of a tokenizer in this exercise were a string tokenizer and a numerical tokenizer.\n\nThe **string tokenizer** was used to breaking down text into tokens. Whereas the **numerical tokenizer**, on the other hand, maps each token to a unique integer ID.","metadata":{}},{"cell_type":"markdown","source":"Q2: Suppose a language model has learned to spell, e.g., after the prefix \"The word dog is spelled d o g\". Will it then already know how to spell any other word, or does it have to re-learn spelling for each word? Why or why not? (For example, try tokenizing the phrase \"The word walking is spelled: w a\" and then asking the LM to complete it.)","metadata":{}},{"cell_type":"markdown","source":"While a language model have learn how to spell, I doubt that it will know how to spell every word there are. The reason I believe for this is because \nWhen a language model is trained, it learns to predict the next word or character given a sequence of previous words or characters. It is possible that during training, the model sees many examples of words in context, and it learns to associate each word with its corresponding spelling pattern. However, the model does not explicitly learn a separate spelling rule for each word. To add to that, they can  generalize from patterns in the data, but it might not be able to memorize specific spellings for every word. They may be able to predict spellings based on common patterns and contexts, but their performance can vary depending on the complexity and frequency of the words encountered during training. ","metadata":{}},{"cell_type":"markdown","source":"Q3: Suppose you made a typo in your input. Explain what the tokenizer we used in this notebook will do with the new input. (Go back and try an input with a typo to see what happens.)","metadata":{}},{"cell_type":"markdown","source":"If I make a typo in your input when using the tokenizer we used in this notebook, the tokenizer will treat the typo as a separate token. The tokenizer we used in this notebook is a WordPiece tokenizer, which breaks down text into subwords based on a fixed vocabulary of tokens. If the tokenizer encounters a word or subword that is not in its vocabulary, it will break down the word into the smallest possible subwords that are in its vocabulary.\n","metadata":{}}]}