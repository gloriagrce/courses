{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Regression Interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a dataset with just a single feature `x` and continuous outcome variable `y`. We'll have a few possible datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'temps'\n",
    "\n",
    "if DATASET == 'toy':\n",
    "    x = np.array([0, 1, 2, 3])[:, np.newaxis]\n",
    "    y_true = np.array([-1, .5, 2.0, 3.5])[:, np.newaxis]\n",
    "if DATASET == 'toy2':\n",
    "    x = np.array([0, 1, 2, 3])[:, np.newaxis]\n",
    "    y_true = np.array([-1, .5, 2.0, 25])[:, np.newaxis]\n",
    "elif DATASET == \"temps\":\n",
    "    data = pd.read_csv(\"https://data.giss.nasa.gov/gistemp/graphs_v4/graph_data/Global_Mean_Estimates_based_on_Land_and_Ocean_Data/graph.csv\", skiprows=1)\n",
    "    # Shape x to be items-by-features\n",
    "    x = data.iloc[:, 0].values.astype(np.float32)[:, np.newaxis]\n",
    "    # scale x to a reasonable range\n",
    "    x -= 1880.0\n",
    "    x /= 100.\n",
    "    y_true = data.iloc[:, 1].values.astype(np.float32)[:, np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(x) < 50:\n",
    "    plt.scatter(x, y_true)\n",
    "else:\n",
    "    plt.plot(x, y_true)\n",
    "print(x.shape, y_true.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates some features that we'll need later. Note that this is hardcoding a linear layer with ReLU activations.\n",
    "\n",
    "After you have explored the potential of the ReLU regression, try switching `activation` to None, re-running all cells, and trying to fit the same data. What does this say about the importance of activation functions in neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = 'relu'\n",
    "# a simple hardcoded linear layer\n",
    "pretend_first_layer_weights = np.array([[1.0, 1.0, 1.0]])\n",
    "pretend_first_layer_bias = np.array([0.0, -0.5, -1.0])\n",
    "pretend_first_layer_out = x @ pretend_first_layer_weights + pretend_first_layer_bias\n",
    "\n",
    "if activation == 'relu':\n",
    "    # a rectifier\n",
    "    pretend_first_layer_activations = np.maximum(pretend_first_layer_out, 0.0)\n",
    "else:\n",
    "    pretend_first_layer_activations = pretend_first_layer_out\n",
    "f1, f2, f3 = pretend_first_layer_activations.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretend_first_layer_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, f1, label=\"always going up\")\n",
    "plt.plot(x, f2, label=\"hinge at 0.5\")\n",
    "plt.plot(x, f3, label=\"hinge at 1.0\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why might that help us? Well, we can mix them together. **Try adjusting the mixing weights to fit the data.** You'll also need to adjust the bias. You should be able to get MSE to be belowe 0.02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.ioff(): # needed to avoid duplicate plots in notebooks\n",
    "    fig = plt.figure()\n",
    "\n",
    "r = 2.0\n",
    "@widgets.interact(w1=(-r, r), w2=(-r, r), w3=(-r, r), bias=(-1.0, 1.0))\n",
    "def plot_linreg(w1=.1, w2=0.0, w3 = 0.0, bias=0.0):\n",
    "    y_pred = w1 * f1 + w2 * f2 + w3 * f3 + bias\n",
    "    plt.cla()\n",
    "    plt.scatter(x, y_true)\n",
    "    plt.plot(x, y_pred, 'r')\n",
    "\n",
    "    resid = y_true - y_pred[:, np.newaxis]\n",
    "    mse = (resid ** 2).mean()\n",
    "    mae = np.abs(resid).mean()\n",
    "    plt.title(f\"MSE: {mse:.3f}, MAE: {mae:.3f}\")\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've found a good fit, try changing:\n",
    "\n",
    "1. The activation function (e.g., `activation = None`)\n",
    "2. The biases of the pretend hidden layer\n",
    "\n",
    "What do you notice about the kind of functions that you can fit with this architecture?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
